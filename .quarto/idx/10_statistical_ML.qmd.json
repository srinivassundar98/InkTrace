{"title":"10: Statistical analysis","markdown":{"yaml":{"title":"10: Statistical analysis"},"headingText":"Emotion Recognition and Sentiment Analysis","containsRefs":false,"markdown":"\nA recent sentiment analysis study presents a comprehensive breakdown of emotions in a dataset, revealing a stark dominance of neutrality in the discourse examined. The donut chart illustrates the sentiment distribution, showcasing that a\nwhopping 73.9% of the entries maintained a neutral tone.\n\n\n\n```{python}\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Load the dataset\ndata = pd.read_csv('main.csv')\n\n# Create a bar graph for emotions using Plotly\nemotion_counts = data['emotion'].value_counts().reset_index()\nemotion_counts.columns = ['emotion', 'count']\nfig_bar = px.bar(emotion_counts, x='emotion', y='count', title='Distribution of Emotions',\n                 labels={'count': 'Count', 'emotion': 'Emotion'}, color='emotion',\n                 color_continuous_scale=px.colors.sequential.Viridis)\nfig_bar.update_layout(xaxis_title='Emotion', yaxis_title='Count', showlegend=False)\nfig_bar.show()\n```\n\n```{python}\n# Create a donut chart for sentiment using Plotly\nsentiment_counts = data['sentiment'].value_counts().reset_index()\nsentiment_counts.columns = ['sentiment', 'count']\nfig_donut = px.pie(sentiment_counts, values='count', names='sentiment', title='Distribution of Sentiments',\n                   hole=0.4, color='sentiment', color_discrete_sequence=px.colors.sequential.RdBu)\nfig_donut.update_traces(textposition='inside', textinfo='percent+label')\nfig_donut.show()\n```\n\nRelationship between Sentiment and Emotion Confidence Scores\nThe plot likely represents the confidence levels of sentiment and emotion predictions, with each point color-coded by sentiment category. The spread and clustering of points can help assess the precision of your sentiment analysis models and identify which emotions or sentiments are predicted with more confidence, providing insights into the reliability of different aspects of your sentiment analysis framework.\n```{python}\nimport pandas as pd\nimport plotly.express as px\n\n# Load the dataset\ndata = pd.read_csv('main.csv')\n\n# Histogram of Sentiment Confidence Values\nfig_hist_sentiment = px.histogram(data, x='sentiment_score', color='sentiment',\n                                  title='Distribution of Sentiment Confidence Scores',\n                                  labels={'sentiment_score': 'Confidence Score'},\n                                  opacity=0.8)\nfig_hist_sentiment.show()\n```\n\n```{python}\n# Scatter Plot of Sentiment vs Emotion Confidence\nfig_scatter = px.scatter(data, x='sentiment_score', y='emotion_score', color='sentiment',\n                         title='Relationship between Sentiment and Emotion Confidence Scores',\n                         labels={'sentiment_score': 'Sentiment Confidence', 'emotion_score': 'Emotion Confidence'})\nfig_scatter.show()\n```\n\nConfidence Scores Distribution for Each Emotion\nThis boxplot distribution shows the variability and central tendency of confidence scores across different emotions. It helps in evaluating the consistency of emotion recognition models, revealing which emotions are recognized with higher confidence and which may require further model tuning or training on more diverse datasets.\n\n```{python}\nimport pandas as pd\nimport plotly.express as px\n\n# Load the dataset\ndata = pd.read_csv('main.csv')\n# Box Plot for Emotion Confidence by Emotion Category\nfig_box = px.box(data, x='emotion', y='emotion_score', color='emotion',\n                 title='Confidence Scores Distribution for Each Emotion',\n                 labels={'emotion_score': 'Emotion Confidence', 'emotion': 'Emotion'})\nfig_box.update_traces(quartilemethod=\"inclusive\")  # or \"exclusive\", depending on how you want to calculate quartiles\nfig_box.show()\n```\n## Classification LSTM Model\n\n### Initial Model: Multinomial Naive Bayes\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n\n# Data loading and preprocessing\ndf = pd.read_csv('main.csv')\ndf = df[['TITLE', 'sentiment', 'emotion','ARTICLE', 'AUTHOR']]\n\n# Column Transformer to handle different feature types\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('tfidf1', TfidfVectorizer(stop_words='english'), 'TITLE'),\n        ('tfidf2', TfidfVectorizer(stop_words='english'), 'ARTICLE'),\n        ('onehot', OneHotEncoder(), ['sentiment', 'emotion'])\n    ])\n\n# Pipeline to vectorize text and train model\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', MultinomialNB())\n])\n\n# Training and prediction\n\n```\n\n### Second Model: Logistic Regression\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\n\n\ndf = pd.read_csv('main.csv')\ndf = df[['TITLE', 'sentiment', 'emotion','ARTICLE', 'AUTHOR']]\n\n# Column Transformer to handle different feature types\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('tfidf1', TfidfVectorizer(stop_words='english'), 'TITLE'),\n        ('tfidf2', TfidfVectorizer(stop_words='english'), 'ARTICLE'),\n        ('onehot', OneHotEncoder(), ['sentiment', 'emotion'])\n    ])\n\n# Pipeline to vectorize text and train model\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(C=1.0, random_state=42))\n])\n\n# Training and prediction\n\n\n\n```\n\n### Final Model: LSTM Neural Network\n\n```python\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom sklearn.model_selection import train_test_split\n\ndf=pd.read_csv('main.csv')\n# Prepare your data\ndf=df[[\"AUTHOR\",\"TITLE\",\"sentiment\",\"emotion\",\"ARTICLE\"]]\ntokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df['TITLE'])\ntitle_sequences = tokenizer.texts_to_sequences(df['TITLE'])\ntitle_padded = pad_sequences(title_sequences, maxlen=50)\n\ntokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df['ARTICLE'])\nart_sequences = tokenizer.texts_to_sequences(df['ARTICLE'])\nart_padded = pad_sequences(art_sequences, maxlen=500)\n\n# One-hot encode sentiment and emotion\nsentiment_encoded = pd.get_dummies(df['sentiment']).values\nemotion_encoded = pd.get_dummies(df['emotion']).values\n\n# Concatenate all features\nimport numpy as np\nfeatures = np.concatenate([title_padded, sentiment_encoded, emotion_encoded,art_padded], axis=1)\n\n# Model building\ninput_layer = Input(shape=(features.shape[1],))\nembedding = Embedding(input_dim=1000, output_dim=64, input_length=features.shape[1])(input_layer)\nlstm_layer = LSTM(64)(embedding)\noutput_layer = Dense(len(df['AUTHOR'].unique()), activation='softmax')(lstm_layer)\nmodel = Model(inputs=input_layer, outputs=output_layer)\n\nmodel.compile(optimizer='adamax', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n#tensorboard_callback = TensorBoard(log_dir=\"./logs\")\n\n# Model training\nlabels = LabelEncoder().fit_transform(df['AUTHOR'])\nX_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\nhistory=model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test),callbacks=[early_stopping])\n```\nComprehensive Input Features: The model leverages a variety of input features including titles and articles, which are tokenized and padded to uniform lengths. Sentiment and emotion are also encoded as additional features, suggesting a multidimensional approach to understanding text data.\n\nUse of LSTM Layer: The LSTM (Long Short-Term Memory) layer is pivotal for processing sequences in the text, capturing temporal dependencies that might be crucial for predicting the author based on writing style and content nuances.\n\nEmbedding Layer: The inclusion of an Embedding layer indicates an effort to transform tokenized words into dense vectors of fixed size, likely aiding in the effective representation of words in a lower-dimensional space.\n\nOutput Layer: The Dense layer with a softmax activation function maps the LSTM outputs to probabilities across the different authors, facilitating multi-class classification.\n\nOptimizer and Loss Function: The model uses the 'adamax' optimizer and 'sparse_categorical_crossentropy', which are suitable choices for multi-class classification problems involving categorical label encoding.\n\nEarly Stopping: Implementation of Early Stopping to prevent overfitting by halting the training process if the validation accuracy does not improve for consecutive epochs, ensuring the model generalizes well on unseen data.\n\nValidation Split: The use of a 30% validation split helps in monitoring the model’s performance on a separate set of data that was not used during training, which is crucial for assessing the generalization capability of the model.\n\n## Neural Network Results\n## Model Plot\n![Model Plot](model_plot.png)\n\n## Model Loss\n![Model Training and Validation Loss](loss.jpg)\n\n## Model Accuracy\n![Model Training and Validation Accuracy](acc.jpg)\n\n## Prediction Matrix\n![Model Training and Validation Accuracy](confusionmat.jpg)\n\nHigh Accuracy: The model achieves a commendable accuracy of 89.2%, indicating strong predictive performance. This suggests that the features and LSTM architecture effectively capture the characteristics necessary to distinguish between authors.\n\nVisualization Tools: The inclusion of plots such as the model loss, accuracy, and a confusion matrix provides visual feedback on training dynamics and classification performance, aiding in quick assessment and iterative improvements.\n\nLoss and Accuracy Trends: From the provided plots, one can discuss trends observed in training and validation loss and accuracy, noting any signs of overfitting or underfitting, and how early stopping may have intervened.","srcMarkdownNoYaml":"\nA recent sentiment analysis study presents a comprehensive breakdown of emotions in a dataset, revealing a stark dominance of neutrality in the discourse examined. The donut chart illustrates the sentiment distribution, showcasing that a\nwhopping 73.9% of the entries maintained a neutral tone.\n\n\n## Emotion Recognition and Sentiment Analysis\n\n```{python}\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Load the dataset\ndata = pd.read_csv('main.csv')\n\n# Create a bar graph for emotions using Plotly\nemotion_counts = data['emotion'].value_counts().reset_index()\nemotion_counts.columns = ['emotion', 'count']\nfig_bar = px.bar(emotion_counts, x='emotion', y='count', title='Distribution of Emotions',\n                 labels={'count': 'Count', 'emotion': 'Emotion'}, color='emotion',\n                 color_continuous_scale=px.colors.sequential.Viridis)\nfig_bar.update_layout(xaxis_title='Emotion', yaxis_title='Count', showlegend=False)\nfig_bar.show()\n```\n\n```{python}\n# Create a donut chart for sentiment using Plotly\nsentiment_counts = data['sentiment'].value_counts().reset_index()\nsentiment_counts.columns = ['sentiment', 'count']\nfig_donut = px.pie(sentiment_counts, values='count', names='sentiment', title='Distribution of Sentiments',\n                   hole=0.4, color='sentiment', color_discrete_sequence=px.colors.sequential.RdBu)\nfig_donut.update_traces(textposition='inside', textinfo='percent+label')\nfig_donut.show()\n```\n\nRelationship between Sentiment and Emotion Confidence Scores\nThe plot likely represents the confidence levels of sentiment and emotion predictions, with each point color-coded by sentiment category. The spread and clustering of points can help assess the precision of your sentiment analysis models and identify which emotions or sentiments are predicted with more confidence, providing insights into the reliability of different aspects of your sentiment analysis framework.\n```{python}\nimport pandas as pd\nimport plotly.express as px\n\n# Load the dataset\ndata = pd.read_csv('main.csv')\n\n# Histogram of Sentiment Confidence Values\nfig_hist_sentiment = px.histogram(data, x='sentiment_score', color='sentiment',\n                                  title='Distribution of Sentiment Confidence Scores',\n                                  labels={'sentiment_score': 'Confidence Score'},\n                                  opacity=0.8)\nfig_hist_sentiment.show()\n```\n\n```{python}\n# Scatter Plot of Sentiment vs Emotion Confidence\nfig_scatter = px.scatter(data, x='sentiment_score', y='emotion_score', color='sentiment',\n                         title='Relationship between Sentiment and Emotion Confidence Scores',\n                         labels={'sentiment_score': 'Sentiment Confidence', 'emotion_score': 'Emotion Confidence'})\nfig_scatter.show()\n```\n\nConfidence Scores Distribution for Each Emotion\nThis boxplot distribution shows the variability and central tendency of confidence scores across different emotions. It helps in evaluating the consistency of emotion recognition models, revealing which emotions are recognized with higher confidence and which may require further model tuning or training on more diverse datasets.\n\n```{python}\nimport pandas as pd\nimport plotly.express as px\n\n# Load the dataset\ndata = pd.read_csv('main.csv')\n# Box Plot for Emotion Confidence by Emotion Category\nfig_box = px.box(data, x='emotion', y='emotion_score', color='emotion',\n                 title='Confidence Scores Distribution for Each Emotion',\n                 labels={'emotion_score': 'Emotion Confidence', 'emotion': 'Emotion'})\nfig_box.update_traces(quartilemethod=\"inclusive\")  # or \"exclusive\", depending on how you want to calculate quartiles\nfig_box.show()\n```\n## Classification LSTM Model\n\n### Initial Model: Multinomial Naive Bayes\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n\n# Data loading and preprocessing\ndf = pd.read_csv('main.csv')\ndf = df[['TITLE', 'sentiment', 'emotion','ARTICLE', 'AUTHOR']]\n\n# Column Transformer to handle different feature types\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('tfidf1', TfidfVectorizer(stop_words='english'), 'TITLE'),\n        ('tfidf2', TfidfVectorizer(stop_words='english'), 'ARTICLE'),\n        ('onehot', OneHotEncoder(), ['sentiment', 'emotion'])\n    ])\n\n# Pipeline to vectorize text and train model\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', MultinomialNB())\n])\n\n# Training and prediction\n\n```\n\n### Second Model: Logistic Regression\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\n\n\ndf = pd.read_csv('main.csv')\ndf = df[['TITLE', 'sentiment', 'emotion','ARTICLE', 'AUTHOR']]\n\n# Column Transformer to handle different feature types\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('tfidf1', TfidfVectorizer(stop_words='english'), 'TITLE'),\n        ('tfidf2', TfidfVectorizer(stop_words='english'), 'ARTICLE'),\n        ('onehot', OneHotEncoder(), ['sentiment', 'emotion'])\n    ])\n\n# Pipeline to vectorize text and train model\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(C=1.0, random_state=42))\n])\n\n# Training and prediction\n\n\n\n```\n\n### Final Model: LSTM Neural Network\n\n```python\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom sklearn.model_selection import train_test_split\n\ndf=pd.read_csv('main.csv')\n# Prepare your data\ndf=df[[\"AUTHOR\",\"TITLE\",\"sentiment\",\"emotion\",\"ARTICLE\"]]\ntokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df['TITLE'])\ntitle_sequences = tokenizer.texts_to_sequences(df['TITLE'])\ntitle_padded = pad_sequences(title_sequences, maxlen=50)\n\ntokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df['ARTICLE'])\nart_sequences = tokenizer.texts_to_sequences(df['ARTICLE'])\nart_padded = pad_sequences(art_sequences, maxlen=500)\n\n# One-hot encode sentiment and emotion\nsentiment_encoded = pd.get_dummies(df['sentiment']).values\nemotion_encoded = pd.get_dummies(df['emotion']).values\n\n# Concatenate all features\nimport numpy as np\nfeatures = np.concatenate([title_padded, sentiment_encoded, emotion_encoded,art_padded], axis=1)\n\n# Model building\ninput_layer = Input(shape=(features.shape[1],))\nembedding = Embedding(input_dim=1000, output_dim=64, input_length=features.shape[1])(input_layer)\nlstm_layer = LSTM(64)(embedding)\noutput_layer = Dense(len(df['AUTHOR'].unique()), activation='softmax')(lstm_layer)\nmodel = Model(inputs=input_layer, outputs=output_layer)\n\nmodel.compile(optimizer='adamax', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n#tensorboard_callback = TensorBoard(log_dir=\"./logs\")\n\n# Model training\nlabels = LabelEncoder().fit_transform(df['AUTHOR'])\nX_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\nhistory=model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test),callbacks=[early_stopping])\n```\nComprehensive Input Features: The model leverages a variety of input features including titles and articles, which are tokenized and padded to uniform lengths. Sentiment and emotion are also encoded as additional features, suggesting a multidimensional approach to understanding text data.\n\nUse of LSTM Layer: The LSTM (Long Short-Term Memory) layer is pivotal for processing sequences in the text, capturing temporal dependencies that might be crucial for predicting the author based on writing style and content nuances.\n\nEmbedding Layer: The inclusion of an Embedding layer indicates an effort to transform tokenized words into dense vectors of fixed size, likely aiding in the effective representation of words in a lower-dimensional space.\n\nOutput Layer: The Dense layer with a softmax activation function maps the LSTM outputs to probabilities across the different authors, facilitating multi-class classification.\n\nOptimizer and Loss Function: The model uses the 'adamax' optimizer and 'sparse_categorical_crossentropy', which are suitable choices for multi-class classification problems involving categorical label encoding.\n\nEarly Stopping: Implementation of Early Stopping to prevent overfitting by halting the training process if the validation accuracy does not improve for consecutive epochs, ensuring the model generalizes well on unseen data.\n\nValidation Split: The use of a 30% validation split helps in monitoring the model’s performance on a separate set of data that was not used during training, which is crucial for assessing the generalization capability of the model.\n\n## Neural Network Results\n## Model Plot\n![Model Plot](model_plot.png)\n\n## Model Loss\n![Model Training and Validation Loss](loss.jpg)\n\n## Model Accuracy\n![Model Training and Validation Accuracy](acc.jpg)\n\n## Prediction Matrix\n![Model Training and Validation Accuracy](confusionmat.jpg)\n\nHigh Accuracy: The model achieves a commendable accuracy of 89.2%, indicating strong predictive performance. This suggests that the features and LSTM architecture effectively capture the characteristics necessary to distinguish between authors.\n\nVisualization Tools: The inclusion of plots such as the model loss, accuracy, and a confusion matrix provides visual feedback on training dynamics and classification performance, aiding in quick assessment and iterative improvements.\n\nLoss and Accuracy Trends: From the provided plots, one can discuss trends observed in training and validation loss and accuracy, noting any signs of overfitting or underfitting, and how early stopping may have intervened."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"10_statistical_ML.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","theme":"cosmo","title":"10: Statistical analysis"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}