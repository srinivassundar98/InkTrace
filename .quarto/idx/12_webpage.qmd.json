{"title":"12: Webpage","markdown":{"yaml":{"title":"12: Webpage","format":"html","editor_options":{"chunk_output_type":"console"},"execute":{"echo":false}},"headingText":"Summarizing Data by Authors","containsRefs":false,"markdown":"\n\n\nThis code snippet identifies missing values in the dataset and summarizes the distribution of publications by authors. It visualizes the results using a Plotly bar graph.\n```{python}\n%%capture\n!pip install nltk\n!pip install gensim\n!pip install pyLDAvis\n\n```\n\n```{python}\nimport pandas as pd\ndata=pd.read_csv('main.csv')\n# Checking for missing values and summarizing the data by authors\nimport plotly.express as px\n\nmissing_values = data.isnull().sum()\nauthor_distribution = data['AUTHOR'].value_counts()\n\nauthor_distribution\nfig = px.bar(author_distribution, \n             x=author_distribution.index, \n             y=author_distribution.values, \n             labels={'x':'Author', 'y':'Count'},\n             title='Distribution of Publications by Author')\nfig.show()\n```\n## Analyzing Temporal Trends in Article Publications\n\nThis code converts the 'DATE' column to datetime format and groups the data by date to count the number of articles. It then plots these counts over time using a line graph to reveal trends.\n\n```{python}\nimport plotly.express as px\n# Convert the 'DATE' column to datetime format\ndata['DATE'] = pd.to_datetime(data['DATE'])\n\n# Group by Date and count the number of articles\ndate_counts = data.groupby('DATE').size().reset_index(name='counts')\n\n# Plotting using Plotly\nfig = px.line(date_counts, x='DATE', y='counts', title='Temporal Trends of Articles', labels={'counts': 'Number of Articles'})\nfig.update_layout(xaxis_title='Date', yaxis_title='Number of Articles')\nfig.show()\n\n```\n\n\n## Monthly Publication Trends by Author\n\nThis code segments the dataset by month and author, creating a stacked bar chart to visualize the number of articles published by each author over time. It effectively illustrates trends and contributions by individual authors.\n\n```{python}\n\n# Group by both Date and Author\nimport pandas as pd\nimport plotly.graph_objects as go\n\n# Load the dataset and prepare it\ndata = pd.read_csv('main.csv')\ndata['DATE'] = pd.to_datetime(data['DATE'])\n\n# Resample data to monthly frequency\ndata['MONTH'] = data['DATE'].dt.to_period('M')\n\n# Group by both Month and Author\nauthor_month_counts = data.groupby(['MONTH', 'AUTHOR']).size().unstack(fill_value=0)\n\n# Create the figure using Plotly\nfig = go.Figure()\nfor author in author_month_counts.columns:\n    fig.add_trace(go.Bar(x=author_month_counts.index.astype(str), y=author_month_counts[author], name=author))\n\nfig.update_layout(\n    barmode='stack',\n    title='Number of Articles by Author (Monthly)',\n    xaxis_title='Month',\n    yaxis_title='Number of Articles',\n    xaxis={'type': 'category'},  # Change this to 'category' to handle the period index as categorical\n    legend_title='Author',\n    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n)\nfig.show()\n\n```\n\n```{python}\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport plotly.express as px\nimport string\n\n# Load the dataset\ndata = pd.read_csv('main.csv')\n\n# Function to clean and prepare text\ndef clean_text(text):\n    # Convert to lower case and remove punctuation\n    text = text.lower()\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    return text\n\n# Apply text cleaning\ndata['cleaned'] = data['cleaned'].apply(clean_text)\n\n# Function to create n-gram frequency plot\ndef plot_ngrams(n, max_features=26):\n    # Create CountVectorizer object for n-grams\n    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english', max_features=max_features)\n    \n    # Fit and transform the data\n    ngrams = vectorizer.fit_transform(data['cleaned'])\n    \n    # Sum up their counts and get feature names\n    sum_ngrams = ngrams.sum(axis=0) \n    ngrams_freq = [(word, sum_ngrams[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n    \n    # Sort n-grams by frequency\n    ngrams_freq = sorted(ngrams_freq, key=lambda x: x[1], reverse=True)\n    words, freqs = zip(*ngrams_freq)\n    \n    # Create a DataFrame\n    df_ngrams = pd.DataFrame({'N-gram': words[1:], 'Frequency': freqs[1:]})\n    \n    # Plot using Plotly\n    if n==1:\n        grams='Uni'\n    elif n==2:\n        grams='Bi'\n    elif n==3:\n        grams='Tri'\n    elif n==4:\n        grams='Quad'\n    fig = px.bar(df_ngrams, x='N-gram', y='Frequency', title=f'Top {25} {grams}-grams', \n                 template='plotly_dark', color='Frequency', color_continuous_scale=px.colors.sequential.Viridis)\n    fig.show()\n\n# Plot unigrams, bigrams, and trigrams\n```\n\n\n## Unigrams\n\n```{python}\nplot_ngrams(1)\n\n\n```\n## Bigrams\n```{python}\nplot_ngrams(2)\n\n\n```\n## Trigrams\n```{python}\nplot_ngrams(3)\n\n```\n## Quadgrams\n```{python}\nplot_ngrams(4)\n```\n\n## Histogram of Word Counts\n\nThis code segment loads text data, calculates word and character counts, and visualizes the distribution of word counts across documents. The histogram of word counts allows us to assess the verbosity or conciseness of the entries in the dataset.\n\n```{python}\nimport pandas as pd\nimport plotly.express as px\n\n# Load the dataset\ndata = pd.read_csv('main.csv')\n\n# Assuming the main text is in a column named 'cleaned'\n# Calculate word count and character count\ndata['word_count'] = data['cleaned'].apply(lambda x: len(str(x).split()))\ndata['char_count'] = data['cleaned'].apply(lambda x: len(str(x)))\n\n# Plotting histograms using Plotly Express\nfig_word_count = px.histogram(data, x='word_count',\n                              title='Histogram of Word Counts',\n                              labels={'word_count': 'Word Count'},\n                              color_discrete_sequence=['skyblue'],\n                              template='plotly_white',\n                              nbins=30)\nfig_word_count.update_layout(bargap=0)\nfig_word_count.show()\n\n```\n\n## Histogram of Character Counts\n\nThis code generates a histogram to visualize the distribution of character counts in the dataset. The visualization helps identify the common length of entries and any outliers, using a clear and concise histogram format.\n\n```{python}\n\nfig_char_count = px.histogram(data, x='char_count',\n                              title='Histogram of Character Counts',\n                              labels={'char_count': 'Character Count'},\n                              color_discrete_sequence=['lightgreen'],\n                              template='plotly_white',\n                              nbins=30)\nfig_char_count.update_layout(bargap=0)\nfig_char_count.show()\n```\n\n## Visualization of POS Tag Frequencies\n\nThis script loads textual data, utilizes the NLTK library to extract and count Part-of-Speech tags, and generates histograms for each POS tag type to analyze their frequencies. The visualizations offer insights into the linguistic structure of the texts.\n```{python}\nimport pandas as pd\nimport nltk\nfrom nltk import pos_tag, word_tokenize\nfrom collections import Counter\nimport plotly.express as px\n\n# Ensure necessary resources are downloaded\nnltk.download('averaged_perceptron_tagger')\nnltk.download('punkt')\nnltk.download('universal_tagset')\n\n# Load the dataset\ndata = pd.read_csv('main.csv')\n\ndef get_pos_tags(texts, tagset='universal'):\n    df_list = []  # List to store each row DataFrame\n    # Iterate over each text item\n    for text in texts:\n        pos_tags = Counter([tag for _, tag in pos_tag(word_tokenize(text), tagset=tagset)])\n        # Create a DataFrame for current text's POS tags and add to list\n        df_list.append(pd.DataFrame([pos_tags]))\n    # Concatenate all DataFrames in the list into a single DataFrame\n    df = pd.concat(df_list, ignore_index=True)\n    df = df.fillna(0).astype(int)\n    return df\n\n# Extract POS tags for the 'cleaned' column\ndf_tags = get_pos_tags(data['cleaned'])\n\n# Plotting each POS tag frequency using Plotly\ndef plot_pos_histograms(df):\n    for column in df.columns:\n        \n        fig = px.histogram(df, x=column, title=f'Histogram of {column} Tags',\n                           labels={column: f'Count of {column}'}, \n                           template='plotly_white', nbins=8)\n        fig.update_traces(marker_color='turquoise', marker_line_width=1.5)\n        fig.update_layout(bargap=0)  # Update bargap to 0 for no gaps\n        \n        fig.show()\n\nplot_pos_histograms(df_tags)\n```\n## Normal Distribution to Title Lengths\n\nThis code calculates the normal distribution based on title lengths and overlays this distribution on a histogram of actual title lengths. This visualization helps assess the normality of title length distribution in the dataset.\n\n```{python}\nimport matplotlib.pyplot as plt\n\ndata['title_length'] = data['TITLE'].apply(len)\n\n# Calculate mean and standard deviation again for title lengths\nmean_title_length = data['title_length'].mean()\nstd_title_length = data['title_length'].std()\nmean_title_length,std_title_length\n# Create a range of values for x (title length) and calculate the normal distribution manually\nimport numpy as np\nx_normal = np.linspace(data['title_length'].min(), data['title_length'].max(), 100)\np_normal = (1 / (std_title_length * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x_normal - mean_title_length) / std_title_length) ** 2)\n\n# Plot the histogram and the normal distribution curve manually\nplt.figure(figsize=(10, 6))\nplt.hist(data['title_length'], bins=30, density=True, alpha=0.6, color='grey')\nplt.plot(x_normal, p_normal, 'r', linewidth=2)\nplt.title('Normal Distribution Fit for Title Lengths')\nplt.xlabel('Title Length (number of characters)')\nplt.ylabel('Density')\nplt.grid(True)\nplt.show()\n```\n# Advanced ML Analytics\n## Topic Modeling\n```{python}\n\nfrom gensim.parsing.preprocessing import preprocess_string\nfrom gensim import corpora, models\nimport gensim\nimport pyLDAvis.gensim_models as gensimvis\nimport pyLDAvis\n\n# Load the dataset\n\n# Preprocess the text\ndef preprocess(text):\n    return preprocess_string(text)\n\n# Apply preprocessing\nprocessed_docs = data['cleaned'].map(preprocess)\n\n# Create a dictionary and corpus needed for Topic Modeling\ndictionary = corpora.Dictionary(processed_docs)\ncorpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n\n# LDA model\nlda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15, random_state=42)\n\n# Display the topics\ntopics = lda_model.print_topics(num_words=5)\nfor topic in topics:\n    print(topic)\n\n# Visualize the topics\npyLDAvis.enable_notebook()\n# Visualize the topics with adjusted size\nlda_display = gensimvis.prepare(lda_model, corpus, dictionary, sort_topics=False, \n                                plot_opts={'width': 800, 'height': 600})  # Adjust width and height as needed\npyLDAvis.display(lda_display)\n```\n## Emotion Recognition and Sentiment Analysis\n```{python}\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Load the dataset\ndata = pd.read_csv('main.csv')\n\n# Create a bar graph for emotions using Plotly\nemotion_counts = data['emotion'].value_counts().reset_index()\nemotion_counts.columns = ['emotion', 'count']\nfig_bar = px.bar(emotion_counts, x='emotion', y='count', title='Distribution of Emotions',\n                 labels={'count': 'Count', 'emotion': 'Emotion'}, color='emotion',\n                 color_continuous_scale=px.colors.sequential.Viridis)\nfig_bar.update_layout(xaxis_title='Emotion', yaxis_title='Count', showlegend=False)\nfig_bar.show()\n\n# Create a donut chart for sentiment using Plotly\nsentiment_counts = data['sentiment'].value_counts().reset_index()\nsentiment_counts.columns = ['sentiment', 'count']\nfig_donut = px.pie(sentiment_counts, values='count', names='sentiment', title='Distribution of Sentiments',\n                   hole=0.4, color='sentiment', color_discrete_sequence=px.colors.sequential.RdBu)\nfig_donut.update_traces(textposition='inside', textinfo='percent+label')\nfig_donut.show()\n\n# Create a sunburst chart with sentiment (inside) and emotion (outside) using Plotly\nfig_sunburst = px.sunburst(data, path=['sentiment', 'emotion'], title='Sunburst Chart of Sentiment and Emotion',\n                           color_continuous_scale='RdBu')\nfig_sunburst.show()\n\n```\n\n```{python}\nimport pandas as pd\nimport plotly.express as px\n\n# Load the dataset\ndata = pd.read_csv('main.csv')\n\n# Histogram of Sentiment Confidence Values\nfig_hist_sentiment = px.histogram(data, x='sentiment_score', color='sentiment',\n                                  title='Distribution of Sentiment Confidence Scores',\n                                  labels={'sentiment_score': 'Confidence Score'},\n                                  opacity=0.8)\nfig_hist_sentiment.show()\n\n# Scatter Plot of Sentiment vs Emotion Confidence\nfig_scatter = px.scatter(data, x='sentiment_score', y='emotion_score', color='sentiment',\n                         title='Relationship between Sentiment and Emotion Confidence Scores',\n                         labels={'sentiment_score': 'Sentiment Confidence', 'emotion_score': 'Emotion Confidence'})\nfig_scatter.show()\n\n# Box Plot for Emotion Confidence by Emotion Category\nfig_box = px.box(data, x='emotion', y='emotion_score', color='emotion',\n                 title='Confidence Scores Distribution for Each Emotion',\n                 labels={'emotion_score': 'Emotion Confidence', 'emotion': 'Emotion'})\nfig_box.update_traces(quartilemethod=\"inclusive\")  # or \"exclusive\", depending on how you want to calculate quartiles\nfig_box.show()\n\n```\n\n## Word Cloud For All Headlines\n![Frequently Occuring Words in all News Headlines](tfidf_unigram_wordcloud.png)\n\n## Word Cloud For Positive Headlines \n![Frequently Occuring Words in positive sentiment News Headlines](tfidf_1gram_positive_wordcloud.png)\n\n## Word Cloud For Negative Headlines \n![Frequently Occuring Words in negative sentiment News Headlines](tfidf_1gram_negative_wordcloud.png)\n\n## Word Cloud For Neutral Headlines \n![Frequently Occuring Words in neutral sentiment News Headlines](tfidf_1gram_neutral_wordcloud.png)\n\n# Deep Learning Classifier\n## Model Plot\n![Model Plot](model_plot.png)\n\n## Model Loss\n![Model Training and Validation Loss](loss.jpg)\n\n## Model Accuracy\n![Model Training and Validation Accuracy](acc.jpg)\n\n## Prediction Matrix\n![Model Training and Validation Accuracy](confusionmat.jpg)\n","srcMarkdownNoYaml":"\n\n## Summarizing Data by Authors\n\nThis code snippet identifies missing values in the dataset and summarizes the distribution of publications by authors. It visualizes the results using a Plotly bar graph.\n```{python}\n%%capture\n!pip install nltk\n!pip install gensim\n!pip install pyLDAvis\n\n```\n\n```{python}\nimport pandas as pd\ndata=pd.read_csv('main.csv')\n# Checking for missing values and summarizing the data by authors\nimport plotly.express as px\n\nmissing_values = data.isnull().sum()\nauthor_distribution = data['AUTHOR'].value_counts()\n\nauthor_distribution\nfig = px.bar(author_distribution, \n             x=author_distribution.index, \n             y=author_distribution.values, \n             labels={'x':'Author', 'y':'Count'},\n             title='Distribution of Publications by Author')\nfig.show()\n```\n## Analyzing Temporal Trends in Article Publications\n\nThis code converts the 'DATE' column to datetime format and groups the data by date to count the number of articles. It then plots these counts over time using a line graph to reveal trends.\n\n```{python}\nimport plotly.express as px\n# Convert the 'DATE' column to datetime format\ndata['DATE'] = pd.to_datetime(data['DATE'])\n\n# Group by Date and count the number of articles\ndate_counts = data.groupby('DATE').size().reset_index(name='counts')\n\n# Plotting using Plotly\nfig = px.line(date_counts, x='DATE', y='counts', title='Temporal Trends of Articles', labels={'counts': 'Number of Articles'})\nfig.update_layout(xaxis_title='Date', yaxis_title='Number of Articles')\nfig.show()\n\n```\n\n\n## Monthly Publication Trends by Author\n\nThis code segments the dataset by month and author, creating a stacked bar chart to visualize the number of articles published by each author over time. It effectively illustrates trends and contributions by individual authors.\n\n```{python}\n\n# Group by both Date and Author\nimport pandas as pd\nimport plotly.graph_objects as go\n\n# Load the dataset and prepare it\ndata = pd.read_csv('main.csv')\ndata['DATE'] = pd.to_datetime(data['DATE'])\n\n# Resample data to monthly frequency\ndata['MONTH'] = data['DATE'].dt.to_period('M')\n\n# Group by both Month and Author\nauthor_month_counts = data.groupby(['MONTH', 'AUTHOR']).size().unstack(fill_value=0)\n\n# Create the figure using Plotly\nfig = go.Figure()\nfor author in author_month_counts.columns:\n    fig.add_trace(go.Bar(x=author_month_counts.index.astype(str), y=author_month_counts[author], name=author))\n\nfig.update_layout(\n    barmode='stack',\n    title='Number of Articles by Author (Monthly)',\n    xaxis_title='Month',\n    yaxis_title='Number of Articles',\n    xaxis={'type': 'category'},  # Change this to 'category' to handle the period index as categorical\n    legend_title='Author',\n    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n)\nfig.show()\n\n```\n\n```{python}\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport plotly.express as px\nimport string\n\n# Load the dataset\ndata = pd.read_csv('main.csv')\n\n# Function to clean and prepare text\ndef clean_text(text):\n    # Convert to lower case and remove punctuation\n    text = text.lower()\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    return text\n\n# Apply text cleaning\ndata['cleaned'] = data['cleaned'].apply(clean_text)\n\n# Function to create n-gram frequency plot\ndef plot_ngrams(n, max_features=26):\n    # Create CountVectorizer object for n-grams\n    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english', max_features=max_features)\n    \n    # Fit and transform the data\n    ngrams = vectorizer.fit_transform(data['cleaned'])\n    \n    # Sum up their counts and get feature names\n    sum_ngrams = ngrams.sum(axis=0) \n    ngrams_freq = [(word, sum_ngrams[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n    \n    # Sort n-grams by frequency\n    ngrams_freq = sorted(ngrams_freq, key=lambda x: x[1], reverse=True)\n    words, freqs = zip(*ngrams_freq)\n    \n    # Create a DataFrame\n    df_ngrams = pd.DataFrame({'N-gram': words[1:], 'Frequency': freqs[1:]})\n    \n    # Plot using Plotly\n    if n==1:\n        grams='Uni'\n    elif n==2:\n        grams='Bi'\n    elif n==3:\n        grams='Tri'\n    elif n==4:\n        grams='Quad'\n    fig = px.bar(df_ngrams, x='N-gram', y='Frequency', title=f'Top {25} {grams}-grams', \n                 template='plotly_dark', color='Frequency', color_continuous_scale=px.colors.sequential.Viridis)\n    fig.show()\n\n# Plot unigrams, bigrams, and trigrams\n```\n\n\n## Unigrams\n\n```{python}\nplot_ngrams(1)\n\n\n```\n## Bigrams\n```{python}\nplot_ngrams(2)\n\n\n```\n## Trigrams\n```{python}\nplot_ngrams(3)\n\n```\n## Quadgrams\n```{python}\nplot_ngrams(4)\n```\n\n## Histogram of Word Counts\n\nThis code segment loads text data, calculates word and character counts, and visualizes the distribution of word counts across documents. The histogram of word counts allows us to assess the verbosity or conciseness of the entries in the dataset.\n\n```{python}\nimport pandas as pd\nimport plotly.express as px\n\n# Load the dataset\ndata = pd.read_csv('main.csv')\n\n# Assuming the main text is in a column named 'cleaned'\n# Calculate word count and character count\ndata['word_count'] = data['cleaned'].apply(lambda x: len(str(x).split()))\ndata['char_count'] = data['cleaned'].apply(lambda x: len(str(x)))\n\n# Plotting histograms using Plotly Express\nfig_word_count = px.histogram(data, x='word_count',\n                              title='Histogram of Word Counts',\n                              labels={'word_count': 'Word Count'},\n                              color_discrete_sequence=['skyblue'],\n                              template='plotly_white',\n                              nbins=30)\nfig_word_count.update_layout(bargap=0)\nfig_word_count.show()\n\n```\n\n## Histogram of Character Counts\n\nThis code generates a histogram to visualize the distribution of character counts in the dataset. The visualization helps identify the common length of entries and any outliers, using a clear and concise histogram format.\n\n```{python}\n\nfig_char_count = px.histogram(data, x='char_count',\n                              title='Histogram of Character Counts',\n                              labels={'char_count': 'Character Count'},\n                              color_discrete_sequence=['lightgreen'],\n                              template='plotly_white',\n                              nbins=30)\nfig_char_count.update_layout(bargap=0)\nfig_char_count.show()\n```\n\n## Visualization of POS Tag Frequencies\n\nThis script loads textual data, utilizes the NLTK library to extract and count Part-of-Speech tags, and generates histograms for each POS tag type to analyze their frequencies. The visualizations offer insights into the linguistic structure of the texts.\n```{python}\nimport pandas as pd\nimport nltk\nfrom nltk import pos_tag, word_tokenize\nfrom collections import Counter\nimport plotly.express as px\n\n# Ensure necessary resources are downloaded\nnltk.download('averaged_perceptron_tagger')\nnltk.download('punkt')\nnltk.download('universal_tagset')\n\n# Load the dataset\ndata = pd.read_csv('main.csv')\n\ndef get_pos_tags(texts, tagset='universal'):\n    df_list = []  # List to store each row DataFrame\n    # Iterate over each text item\n    for text in texts:\n        pos_tags = Counter([tag for _, tag in pos_tag(word_tokenize(text), tagset=tagset)])\n        # Create a DataFrame for current text's POS tags and add to list\n        df_list.append(pd.DataFrame([pos_tags]))\n    # Concatenate all DataFrames in the list into a single DataFrame\n    df = pd.concat(df_list, ignore_index=True)\n    df = df.fillna(0).astype(int)\n    return df\n\n# Extract POS tags for the 'cleaned' column\ndf_tags = get_pos_tags(data['cleaned'])\n\n# Plotting each POS tag frequency using Plotly\ndef plot_pos_histograms(df):\n    for column in df.columns:\n        \n        fig = px.histogram(df, x=column, title=f'Histogram of {column} Tags',\n                           labels={column: f'Count of {column}'}, \n                           template='plotly_white', nbins=8)\n        fig.update_traces(marker_color='turquoise', marker_line_width=1.5)\n        fig.update_layout(bargap=0)  # Update bargap to 0 for no gaps\n        \n        fig.show()\n\nplot_pos_histograms(df_tags)\n```\n## Normal Distribution to Title Lengths\n\nThis code calculates the normal distribution based on title lengths and overlays this distribution on a histogram of actual title lengths. This visualization helps assess the normality of title length distribution in the dataset.\n\n```{python}\nimport matplotlib.pyplot as plt\n\ndata['title_length'] = data['TITLE'].apply(len)\n\n# Calculate mean and standard deviation again for title lengths\nmean_title_length = data['title_length'].mean()\nstd_title_length = data['title_length'].std()\nmean_title_length,std_title_length\n# Create a range of values for x (title length) and calculate the normal distribution manually\nimport numpy as np\nx_normal = np.linspace(data['title_length'].min(), data['title_length'].max(), 100)\np_normal = (1 / (std_title_length * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x_normal - mean_title_length) / std_title_length) ** 2)\n\n# Plot the histogram and the normal distribution curve manually\nplt.figure(figsize=(10, 6))\nplt.hist(data['title_length'], bins=30, density=True, alpha=0.6, color='grey')\nplt.plot(x_normal, p_normal, 'r', linewidth=2)\nplt.title('Normal Distribution Fit for Title Lengths')\nplt.xlabel('Title Length (number of characters)')\nplt.ylabel('Density')\nplt.grid(True)\nplt.show()\n```\n# Advanced ML Analytics\n## Topic Modeling\n```{python}\n\nfrom gensim.parsing.preprocessing import preprocess_string\nfrom gensim import corpora, models\nimport gensim\nimport pyLDAvis.gensim_models as gensimvis\nimport pyLDAvis\n\n# Load the dataset\n\n# Preprocess the text\ndef preprocess(text):\n    return preprocess_string(text)\n\n# Apply preprocessing\nprocessed_docs = data['cleaned'].map(preprocess)\n\n# Create a dictionary and corpus needed for Topic Modeling\ndictionary = corpora.Dictionary(processed_docs)\ncorpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n\n# LDA model\nlda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15, random_state=42)\n\n# Display the topics\ntopics = lda_model.print_topics(num_words=5)\nfor topic in topics:\n    print(topic)\n\n# Visualize the topics\npyLDAvis.enable_notebook()\n# Visualize the topics with adjusted size\nlda_display = gensimvis.prepare(lda_model, corpus, dictionary, sort_topics=False, \n                                plot_opts={'width': 800, 'height': 600})  # Adjust width and height as needed\npyLDAvis.display(lda_display)\n```\n## Emotion Recognition and Sentiment Analysis\n```{python}\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Load the dataset\ndata = pd.read_csv('main.csv')\n\n# Create a bar graph for emotions using Plotly\nemotion_counts = data['emotion'].value_counts().reset_index()\nemotion_counts.columns = ['emotion', 'count']\nfig_bar = px.bar(emotion_counts, x='emotion', y='count', title='Distribution of Emotions',\n                 labels={'count': 'Count', 'emotion': 'Emotion'}, color='emotion',\n                 color_continuous_scale=px.colors.sequential.Viridis)\nfig_bar.update_layout(xaxis_title='Emotion', yaxis_title='Count', showlegend=False)\nfig_bar.show()\n\n# Create a donut chart for sentiment using Plotly\nsentiment_counts = data['sentiment'].value_counts().reset_index()\nsentiment_counts.columns = ['sentiment', 'count']\nfig_donut = px.pie(sentiment_counts, values='count', names='sentiment', title='Distribution of Sentiments',\n                   hole=0.4, color='sentiment', color_discrete_sequence=px.colors.sequential.RdBu)\nfig_donut.update_traces(textposition='inside', textinfo='percent+label')\nfig_donut.show()\n\n# Create a sunburst chart with sentiment (inside) and emotion (outside) using Plotly\nfig_sunburst = px.sunburst(data, path=['sentiment', 'emotion'], title='Sunburst Chart of Sentiment and Emotion',\n                           color_continuous_scale='RdBu')\nfig_sunburst.show()\n\n```\n\n```{python}\nimport pandas as pd\nimport plotly.express as px\n\n# Load the dataset\ndata = pd.read_csv('main.csv')\n\n# Histogram of Sentiment Confidence Values\nfig_hist_sentiment = px.histogram(data, x='sentiment_score', color='sentiment',\n                                  title='Distribution of Sentiment Confidence Scores',\n                                  labels={'sentiment_score': 'Confidence Score'},\n                                  opacity=0.8)\nfig_hist_sentiment.show()\n\n# Scatter Plot of Sentiment vs Emotion Confidence\nfig_scatter = px.scatter(data, x='sentiment_score', y='emotion_score', color='sentiment',\n                         title='Relationship between Sentiment and Emotion Confidence Scores',\n                         labels={'sentiment_score': 'Sentiment Confidence', 'emotion_score': 'Emotion Confidence'})\nfig_scatter.show()\n\n# Box Plot for Emotion Confidence by Emotion Category\nfig_box = px.box(data, x='emotion', y='emotion_score', color='emotion',\n                 title='Confidence Scores Distribution for Each Emotion',\n                 labels={'emotion_score': 'Emotion Confidence', 'emotion': 'Emotion'})\nfig_box.update_traces(quartilemethod=\"inclusive\")  # or \"exclusive\", depending on how you want to calculate quartiles\nfig_box.show()\n\n```\n\n## Word Cloud For All Headlines\n![Frequently Occuring Words in all News Headlines](tfidf_unigram_wordcloud.png)\n\n## Word Cloud For Positive Headlines \n![Frequently Occuring Words in positive sentiment News Headlines](tfidf_1gram_positive_wordcloud.png)\n\n## Word Cloud For Negative Headlines \n![Frequently Occuring Words in negative sentiment News Headlines](tfidf_1gram_negative_wordcloud.png)\n\n## Word Cloud For Neutral Headlines \n![Frequently Occuring Words in neutral sentiment News Headlines](tfidf_1gram_neutral_wordcloud.png)\n\n# Deep Learning Classifier\n## Model Plot\n![Model Plot](model_plot.png)\n\n## Model Loss\n![Model Training and Validation Loss](loss.jpg)\n\n## Model Accuracy\n![Model Training and Validation Accuracy](acc.jpg)\n\n## Prediction Matrix\n![Model Training and Validation Accuracy](confusionmat.jpg)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"12_webpage.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","theme":"cosmo","title":"12: Webpage","editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}